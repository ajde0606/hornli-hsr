{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4899b6a-943a-4a4e-9586-1c7521d40a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/janiceyan/work/data/hsr\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from hsr.config import *\n",
    "from hsr.loading import winsorize_and_standardize_descriptor, styles\n",
    "from hsr.regression import cross_sectional_regression_one_day, concat_loadings, _normalize_cap_weights\n",
    "from hsr.analysis import compute_risk_from_panels_rolling, factor_variance_explained_per_asset,factor_variance_explained_portfolio,calc_realized_vol\n",
    "print(DEFAULT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307161c",
   "metadata": {},
   "source": [
    "# calculate loadings"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7114b0ac",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "To construct style loadings, you will need: \n",
    "$HSR_PATH/intermediate/descriptor.parquet\n",
    "\n",
    "If you don't have this file, run:\n",
    "\n",
    "$ curl -L -o $HSR_PATH/intermediate/descriptor.parquet http://hornliquant.com/api/risk-model/intermediate/files/descriptor.parquet\n",
    "\n",
    "The following section does the following 2 steps:\n",
    "1. read raw descriptor value, winsorize and standardize cross-sectional\n",
    "2. group descriptors to style, do winsorization and standardization again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc2773e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winsorizing and standardizing beta\n",
      "winsorizing and standardizing volatility_dsd\n",
      "winsorizing and standardizing volatility_hilo\n",
      "winsorizing and standardizing momentum_rs\n",
      "winsorizing and standardizing size\n",
      "winsorizing and standardizing nonlinear_size\n",
      "winsorizing and standardizing trading_activity_annual\n",
      "winsorizing and standardizing trading_activity_quarter\n",
      "winsorizing and standardizing trading_activity_5y\n",
      "winsorizing and standardizing growth_payo\n",
      "winsorizing and standardizing growth_vcap\n",
      "winsorizing and standardizing growth_agro\n",
      "winsorizing and standardizing growth_egro\n",
      "winsorizing and standardizing growth_dele\n",
      "winsorizing and standardizing earnings_yield_trailing_12m\n",
      "winsorizing and standardizing value_bp\n",
      "winsorizing and standardizing value_sp\n",
      "winsorizing and standardizing value_cfp\n",
      "winsorizing and standardizing earnings_variability_vern\n",
      "winsorizing and standardizing earnings_variability_vsales\n",
      "winsorizing and standardizing earnings_variability_vcfs\n",
      "winsorizing and standardizing earnings_variability_cfaccruals\n",
      "winsorizing and standardizing leverage_mlev\n",
      "winsorizing and standardizing leverage_blev\n",
      "winsorizing and standardizing leverage_dtoa\n",
      "winsorizing and standardizing dividend_yield_annual\n",
      "winsorizing and standardizing management_quality_ag\n",
      "winsorizing and standardizing management_quality_sg\n",
      "winsorizing and standardizing management_quality_cg\n",
      "winsorizing and standardizing management_quality_ca\n"
     ]
    }
   ],
   "source": [
    "# step 1\n",
    "descriptor_df = pd.read_parquet(os.path.join(DEFAULT_PATH, \"intermediate/descriptor.parquet\"))\n",
    "out_fn = os.path.join(DEFAULT_PATH, \"intermediate/cap_weights.parquet\")\n",
    "mcap = descriptor_df[descriptor_df[\"descriptor\"] == \"market_cap\"].pivot(\n",
    "    index=date_col, columns=identifier, values=\"val\"\n",
    ")\n",
    "mcap.to_parquet(out_fn)\n",
    "\n",
    "zscore_dfs = []\n",
    "for descriptor in descriptor_df[\"descriptor\"].unique():\n",
    "    if descriptor == \"market_cap\": continue  # market_cap is not a descriptor\n",
    "\n",
    "    df = descriptor_df[descriptor_df[\"descriptor\"] == descriptor].pivot(\n",
    "        index=date_col, columns=identifier, values=\"val\"\n",
    "    )\n",
    "        \n",
    "    zscore_df = winsorize_and_standardize_descriptor(df, descriptor, mcap)\n",
    "    zscore_dfs.append(zscore_df)\n",
    "zscore_df = pd.concat(zscore_dfs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6c86cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winsorizing and standardizing beta\n",
      "winsorizing and standardizing volatility\n",
      "winsorizing and standardizing momentum\n",
      "winsorizing and standardizing size\n",
      "winsorizing and standardizing nonlinear_size\n",
      "winsorizing and standardizing trading_activity\n",
      "winsorizing and standardizing growth\n",
      "winsorizing and standardizing earnings_yield\n",
      "winsorizing and standardizing value\n",
      "winsorizing and standardizing earnings_variability\n",
      "winsorizing and standardizing leverage\n",
      "winsorizing and standardizing dividend_yield\n"
     ]
    }
   ],
   "source": [
    "# step 2\n",
    "loading_dfs = []\n",
    "for style in styles:\n",
    "    cols = [s for s in zscore_df.columns if s.startswith(style)]\n",
    "    df = zscore_df[cols].mean(axis=1)\n",
    "    df.name = style\n",
    "    df = df.reset_index().pivot(index=date_col, columns=identifier, values=style)\n",
    "    df = winsorize_and_standardize_descriptor(df, style, mcap)\n",
    "    loading_dfs.append(df)\n",
    "loading_df = pd.concat(loading_dfs, axis=1)\n",
    "out_fn = os.path.join(DEFAULT_PATH, \"intermediate/loadings.parquet\")\n",
    "loading_df.to_parquet(out_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3140ea6",
   "metadata": {},
   "source": [
    "# Construct factor returns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da70ea3a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Next, let's compute factor return by running regression with\n",
    "y = total return\n",
    "x = (style, industry, country)\n",
    "\n",
    "To run the regression, you will need the following files:\n",
    "1. $HSR_PATH/intermediate/simple_return.parquet\n",
    "2. $HSR_PATH/intermediate/industry_one_hot.parquet\n",
    "3. $HSR_PATH/intermediate/loadings.parquet\n",
    "4. $HSR_PATH/intermediate/cap_weights.parquet\n",
    "\n",
    "if you have anything missing, run\n",
    "$ export filename=simple_return\n",
    "$ curl -L -o $HSR_PATH/intermediate/$filename.parquet http://hornliquant.com/api/risk-model/intermediate/files/$filename.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7f51a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "simple_ret_df = pd.read_parquet(os.path.join(DEFAULT_PATH, \"intermediate/simple_return.parquet\"))\n",
    "industry_df = pd.read_parquet(os.path.join(DEFAULT_PATH, \"intermediate/industry_one_hot.parquet\"))\n",
    "loading_df = pd.read_parquet(os.path.join(DEFAULT_PATH, \"intermediate/loadings.parquet\")).reset_index()\n",
    "mkt_cap = pd.read_parquet(os.path.join(DEFAULT_PATH, \"intermediate/cap_weights.parquet\"))\n",
    "\n",
    "all_dates = np.intersect1d(simple_ret_df.index, loading_df[date_col].unique())\n",
    "all_tickers = np.intersect1d(simple_ret_df.columns, loading_df[identifier].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edb84968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize data and run by day\n",
    "\n",
    "def run_one_regression(date, all_tickers,\n",
    "                       simple_ret_df,\n",
    "                       industry_df,\n",
    "                       loading_df,\n",
    "                       mkt_cap\n",
    "                       ):\n",
    "\n",
    "    def _empty(S):\n",
    "        for col in S.columns:\n",
    "            if S[col].abs().sum() == 0:\n",
    "                # print(col)\n",
    "                return True\n",
    "        return False    \n",
    "    r = simple_ret_df.loc[date, all_tickers].dropna()\n",
    "    tickers_ = r.index\n",
    "    D_ind = industry_df.loc[tickers_]\n",
    "    X_style = loading_df[loading_df[date_col] == date].drop(columns=[date_col])\n",
    "    X_style = X_style.set_index(identifier).loc[tickers_].fillna(0.)\n",
    "    C_cty = pd.DataFrame(np.ones(len(tickers_)), index=tickers_, columns=[\"country\"])\n",
    "    mcap = mkt_cap.loc[date, tickers_].fillna(0.)\n",
    "\n",
    "    if _empty(X_style): \n",
    "        # print(f\"empty style factor detected, skipping {date}\")\n",
    "        return None, X_style\n",
    "    # print(f\"Processing {date}\")\n",
    "\n",
    "    f_ret, e, _ = cross_sectional_regression_one_day(\n",
    "        r, X_style, D_ind, C_cty, mcap,\n",
    "        hsigma=prev_sv.pow(0.5) if 'prev_sv' in locals() else None,\n",
    "    )\n",
    "    f_ret.name = date\n",
    "    e.name = date\n",
    "    return f_ret, e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46775eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "\n",
    "factor_returns = []\n",
    "specific_returns = []\n",
    "for date in all_dates[-252:]:   # run one year\n",
    "    f_ret, e = run_one_regression(date, all_tickers,\n",
    "                                  simple_ret_df,\n",
    "                                  industry_df,\n",
    "                                  loading_df,\n",
    "                                  mkt_cap)\n",
    "    if f_ret is None: continue\n",
    "\n",
    "    factor_returns.append(f_ret)\n",
    "    specific_returns.append(e)\n",
    "\n",
    "factor_return_df = pd.concat(factor_returns, axis=1).T\n",
    "specific_return_df = pd.concat(specific_returns, axis=1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f71a56",
   "metadata": {},
   "source": [
    "# Compute Factor Covaraince & Specific Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "556de8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm using s&p as regime proxy. you can define your own.\n",
    "regime_proxy = None\n",
    "regime_proxy_fn =os.path.join(DEFAULT_PATH, \"intermediate/sp.csv\") \n",
    "if os.path.exists(regime_proxy_fn):\n",
    "    regime_proxy = pd.read_csv(regime_proxy_fn, header=None)\n",
    "    regime_proxy = pd.Series(regime_proxy[1].values,\n",
    "                             index=pd.to_datetime(regime_proxy[0].values),\n",
    "                             name=\"regime\")\n",
    "\n",
    "factor_cov, regime_multiplier, specific_var = compute_risk_from_panels_rolling(\n",
    "        factor_return_df,\n",
    "        specific_return_df.T,\n",
    "        regime_proxy=regime_proxy,\n",
    "        cov_format=\"long\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021313f",
   "metadata": {},
   "source": [
    "# Some analysis measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34f49204",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"2025-10-17\" # enter a date\n",
    "\n",
    "country_df = pd.DataFrame(1, index=industry_df.index, columns=[\"country\"])\n",
    "X_s = loading_df[loading_df[date_col] == date].drop(columns=[date_col])\n",
    "X_s = X_s.set_index(identifier)\n",
    "        \n",
    "Sigma_f_t = factor_cov[factor_cov[\"date\"] == date]\n",
    "Sigma_f_t = Sigma_f_t.pivot(index=\"row_factor\",\n",
    "                            columns=\"col_factor\",\n",
    "                            values=\"value\")\n",
    "spec_var_t = specific_var.T.loc[date]\n",
    "\n",
    "idx = spec_var_t.index\n",
    "X_t, all_cols, _, _, _ = concat_loadings(X_s, industry_df, country_df, idx)\n",
    "X_t = pd.DataFrame(X_t, index=idx, columns=all_cols)\n",
    "\n",
    "mcap = mkt_cap.loc[date, X_s.index].fillna(0.)\n",
    "mcap = _normalize_cap_weights(mcap, X_s.index).fillna(0.)\n",
    "realized_ret = simple_ret_df.loc[: date, X_s.index].fillna(0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0736e723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "R2_factor_explained",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e870f5f5-d2a0-4dd8-83e5-b922c157a358",
       "rows": [
        [
         "count",
         "2563.0"
        ],
        [
         "mean",
         "0.9054416790278306"
        ],
        [
         "std",
         "0.07725464679447479"
        ],
        [
         "min",
         "0.28311723919975795"
        ],
        [
         "25%",
         "0.8756761634062238"
        ],
        [
         "50%",
         "0.9236217573229931"
        ],
        [
         "75%",
         "0.9582676448782668"
        ],
        [
         "max",
         "0.9935170355523486"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "count    2563.000000\n",
       "mean        0.905442\n",
       "std         0.077255\n",
       "min         0.283117\n",
       "25%         0.875676\n",
       "50%         0.923622\n",
       "75%         0.958268\n",
       "max         0.993517\n",
       "Name: R2_factor_explained, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_by_asset = factor_variance_explained_per_asset(X_t, Sigma_f_t, spec_var_t)\n",
    "r2_by_asset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6005b4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cap weighted portfolio R2: 0.93, implied vol: 0.0087, realized vol: 0.0133\n"
     ]
    }
   ],
   "source": [
    "r2_mcap_port, _, _, mcap_implied_tot = factor_variance_explained_portfolio(mcap, X_t, Sigma_f_t, spec_var_t)\n",
    "mcap_realized_tot = calc_realized_vol(mcap, realized_ret, window_size=252)\n",
    "\n",
    "print(\"cap weighted portfolio R2: %.2f, implied vol: %.4f, realized vol: %.4f\" % \n",
    "      (r2_mcap_port, np.sqrt(mcap_implied_tot), np.sqrt(mcap_realized_tot)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550c06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "warren",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
