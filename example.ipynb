{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4899b6a-943a-4a4e-9586-1c7521d40a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/janiceyan/work/data/hsr\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from hsr.config import *\n",
    "from hsr.loading import winsorize_and_standardize_descriptor, styles\n",
    "from hsr.regression import cross_sectional_regression_one_day, concat_loadings\n",
    "from hsr.analysis import compute_risk_from_panels_rolling, factor_variance_explained_per_asset\n",
    "\n",
    "print(DEFAULT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307161c",
   "metadata": {},
   "source": [
    "# calculate loadings"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7114b0ac",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "To construct style loadings, you will need: \n",
    "$HSR_PATH/intermediate/descriptor.parquet\n",
    "\n",
    "If you don't have this file, run:\n",
    "\n",
    "$ mkdir -p $HSR_PATH/intermediate\n",
    "$ curl -L -o $HSR_PATH/intermediate/descriptor.parquet http://hornliquant.com/api/risk-model/intermediate/files/descriptor.parquet\n",
    "\n",
    "The following section does the following 2 steps:\n",
    "1. read raw descriptor value, winsorize and standardize cross-sectional\n",
    "2. group descriptors to style, do winsorization and standardization again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc2773e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winsorizing and standardizing beta\n",
      "winsorizing and standardizing volatility_dsd\n",
      "winsorizing and standardizing volatility_hilo\n",
      "winsorizing and standardizing momentum_rs\n",
      "winsorizing and standardizing size\n",
      "winsorizing and standardizing nonlinear_size\n",
      "winsorizing and standardizing trading_activity_annual\n",
      "winsorizing and standardizing trading_activity_quarter\n",
      "winsorizing and standardizing trading_activity_5y\n",
      "winsorizing and standardizing growth_payo\n",
      "winsorizing and standardizing growth_vcap\n",
      "winsorizing and standardizing growth_agro\n",
      "winsorizing and standardizing growth_egro\n",
      "winsorizing and standardizing growth_dele\n",
      "winsorizing and standardizing earnings_yield_trailing_12m\n",
      "winsorizing and standardizing value_bp\n",
      "winsorizing and standardizing value_sp\n",
      "winsorizing and standardizing value_cfp\n",
      "winsorizing and standardizing earnings_variability_vern\n",
      "winsorizing and standardizing earnings_variability_vsales\n",
      "winsorizing and standardizing earnings_variability_vcfs\n",
      "winsorizing and standardizing earnings_variability_cfaccruals\n",
      "winsorizing and standardizing leverage_mlev\n",
      "winsorizing and standardizing leverage_blev\n",
      "winsorizing and standardizing leverage_dtoa\n",
      "winsorizing and standardizing dividend_yield_annual\n",
      "winsorizing and standardizing management_quality_ag\n",
      "winsorizing and standardizing management_quality_sg\n",
      "winsorizing and standardizing management_quality_cg\n",
      "winsorizing and standardizing management_quality_ca\n"
     ]
    }
   ],
   "source": [
    "# step 1\n",
    "descriptor_df = pd.read_parquet(os.path.join(DEFAULT_PATH, \"intermediate/descriptor.parquet\"))\n",
    "\n",
    "zscore_dfs = []\n",
    "for descriptor in descriptor_df[\"descriptor\"].unique():\n",
    "    if descriptor == \"market_cap\": continue  # market_cap is not a descriptor\n",
    "\n",
    "    df = descriptor_df[descriptor_df[\"descriptor\"] == descriptor].pivot(\n",
    "        index=date_col, columns=identifier, values=\"val\"\n",
    "    )\n",
    "        \n",
    "    zscore_df = winsorize_and_standardize_descriptor(df, descriptor)\n",
    "    zscore_dfs.append(zscore_df)\n",
    "zscore_df = pd.concat(zscore_dfs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f6c86cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winsorizing and standardizing beta\n",
      "winsorizing and standardizing volatility\n",
      "winsorizing and standardizing momentum\n",
      "winsorizing and standardizing size\n",
      "winsorizing and standardizing nonlinear_size\n",
      "winsorizing and standardizing trading_activity\n",
      "winsorizing and standardizing growth\n",
      "winsorizing and standardizing earnings_yield\n",
      "winsorizing and standardizing value\n",
      "winsorizing and standardizing earnings_variability\n",
      "winsorizing and standardizing leverage\n",
      "winsorizing and standardizing dividend_yield\n"
     ]
    }
   ],
   "source": [
    "# step 2\n",
    "loading_dfs = []\n",
    "for style in styles:\n",
    "    cols = [s for s in zscore_df.columns if s.startswith(style)]\n",
    "    df = zscore_df[cols].mean(axis=1)\n",
    "    df.name = style\n",
    "    df = df.reset_index().pivot(index=date_col, columns=identifier, values=style)\n",
    "    df = winsorize_and_standardize_descriptor(df, style)\n",
    "    loading_dfs.append(df)\n",
    "loading_df = pd.concat(loading_dfs, axis=1)\n",
    "out_fn = os.path.join(DEFAULT_PATH, \"intermediate/loadings.parquet\")\n",
    "loading_df.to_parquet(out_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3140ea6",
   "metadata": {},
   "source": [
    "# Construct factor returns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da70ea3a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Next, let's compute factor return by running regression with\n",
    "y = total return\n",
    "x = (style, industry, country)\n",
    "\n",
    "To run the regression, you will need the following files:\n",
    "1. $HSR_PATH/intermediate/simple_return.parquet\n",
    "2. $HSR_PATH/intermediate/industry_one_hot.parquet\n",
    "3. $HSR_PATH/intermediate/loadings.parquet\n",
    "4. $HSR_PATH/intermediate/cap_weights.parquet\n",
    "\n",
    "if you have anything missing, run\n",
    "$ export filename=simple_return\n",
    "$ curl -L -o $HSR_PATH/intermediate/$filename.parquet http://hornliquant.com/api/risk-model/intermediate/files/$filename.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7f51a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "simple_ret_df = pd.read_parquet(os.path.join(DEFAULT_PATH, \"intermediate/simple_return.parquet\"))\n",
    "industry_df = pd.read_parquet(os.path.join(DEFAULT_PATH, \"intermediate/industry_one_hot.parquet\"))\n",
    "loading_df = pd.read_parquet(os.path.join(DEFAULT_PATH, \"intermediate/loadings.parquet\")).reset_index()\n",
    "mkt_cap = pd.read_parquet(os.path.join(DEFAULT_PATH, \"intermediate/cap_weights.parquet\"))\n",
    "\n",
    "all_dates = np.intersect1d(simple_ret_df.index, loading_df[date_col].unique())\n",
    "all_tickers = np.intersect1d(simple_ret_df.columns, loading_df[identifier].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb84968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize data and run by day\n",
    "\n",
    "def run_one_regression(date, all_tickers,\n",
    "                       simple_ret_df,\n",
    "                       industry_df,\n",
    "                       loading_df,\n",
    "                       mkt_cap\n",
    "                       ):\n",
    "\n",
    "    def _empty(S):\n",
    "        for col in S.columns:\n",
    "            if S[col].abs().sum() == 0:\n",
    "                print(col)\n",
    "                return True\n",
    "        return False    \n",
    "    r = simple_ret_df.loc[date, all_tickers].dropna()\n",
    "    tickers_ = r.index\n",
    "    D_ind = industry_df.loc[tickers_]\n",
    "    X_style = loading_df[loading_df[date_col] == date].drop(columns=[date_col])\n",
    "    X_style = X_style.set_index(identifier).loc[tickers_].fillna(0.)\n",
    "    C_cty = pd.DataFrame(np.ones(len(tickers_)), index=tickers_, columns=[\"country\"])\n",
    "    mcap = mkt_cap.loc[date, tickers_].fillna(0.)\n",
    "\n",
    "    if _empty(X_style): \n",
    "        print(f\"empty style factor detected, skipping {date}\")\n",
    "        return None, X_style\n",
    "    # print(f\"Processing {date}\")\n",
    "\n",
    "    f_ret, e = cross_sectional_regression_one_day(\n",
    "        r, X_style, D_ind, C_cty, mcap,\n",
    "        hsigma=prev_sv.pow(0.5) if 'prev_sv' in locals() else None,\n",
    "    )\n",
    "    f_ret.name = date\n",
    "    e.name = date\n",
    "    return f_ret, e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46775eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "\n",
    "factor_returns = []\n",
    "specific_returns = []\n",
    "for date in all_dates:\n",
    "    f_ret, e = run_one_regression(date, all_tickers,\n",
    "                                  simple_ret_df,\n",
    "                                  industry_df,\n",
    "                                  loading_df,\n",
    "                                  mkt_cap)\n",
    "    if f_ret is None: continue\n",
    "\n",
    "    factor_returns.append(f_ret)\n",
    "    specific_returns.append(e)\n",
    "\n",
    "factor_return_df = pd.concat(factor_returns, axis=1).T\n",
    "specific_return_df = pd.concat(specific_returns, axis=1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f71a56",
   "metadata": {},
   "source": [
    "# Compute Factor Covaraince & Specific Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "556de8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm using s&p as regime proxy. you can define your own.\n",
    "regime_proxy = None\n",
    "regime_proxy_fn =os.path.join(DEFAULT_PATH, \"intermediate/sp.csv\") \n",
    "if os.path.exists(regime_proxy_fn):\n",
    "    regime_proxy = pd.read_csv(regime_proxy_fn, header=None)\n",
    "    regime_proxy = pd.Series(regime_proxy[1].values,\n",
    "                             index=pd.to_datetime(regime_proxy[0].values),\n",
    "                             name=\"regime\")\n",
    "\n",
    "factor_cov, regime_multiplier, specific_var = compute_risk_from_panels_rolling(\n",
    "        factor_return_df,\n",
    "        specific_return_df.T,\n",
    "        regime_proxy=regime_proxy,\n",
    "        cov_format=\"long\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6021313f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "warren",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
